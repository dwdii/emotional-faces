{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import Image\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from scipy import misc\n",
    "import string\n",
    "import time\n",
    "\n",
    "# Set some Theano config before initializing\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=cpu,floatX=float32,allow_gc=False,openmp=True\"\n",
    "import theano\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import emotion_model\n",
    "import dwdii_transforms\n",
    "\n",
    "random.seed(20275)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "floatX: float32\n",
      "mode: FAST_RUN\n",
      "openmp: True\n",
      "allow_gc: False\n"
     ]
    }
   ],
   "source": [
    "print \"device:\", theano.config.device\n",
    "print \"floatX:\",  theano.config.floatX\n",
    "print \"mode:\", theano.config.mode\n",
    "print \"openmp:\", theano.config.openmp\n",
    "print \"allow_gc:\", theano.config.allow_gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imagePath = \"/root/facial_expressions/images\"\n",
    "dataPath = \"/root/facial_expressions/data/legend.csv\"\n",
    "imgResize = (150, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['legend.csv', '500_picts_satz.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/root/facial_expressions/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#emoMetaData = dwdii_transforms.load_training_metadata(dataPath, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(emoMetaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000: King_Abdullah_II_0001.jpg\n",
      "0.109709: Lleyton_Hewitt_0041.jpg\n",
      "0.219419: Dwarakish_30.jpg\n",
      "0.219419: Megawati_Sukarnoputri_0018.jpg\n",
      "0.329128: HrithikRoshan_174.jpg\n",
      "0.329128: Valerie_Thwaites_0001.jpg\n",
      "0.438837: HrithikRoshan_115.jpg\n",
      "0.438837: Dwarakish_180.jpg\n",
      "0.438837: HrithikRoshan_3.jpg\n",
      "0.438837: George_W_Bush_0422.jpg\n",
      "0.548546: Serena_Williams_0015.jpg\n",
      "0.658256: Bob_Riley_0001.jpg\n",
      "0.767965: Lindsay_Davenport_0012.jpg\n",
      "0.877674: HrithikRoshan_112.jpg\n",
      "0.877674: Sergio_Vieira_De_Mello_0002.jpg\n",
      "0.987383: Trent_Lott_0016.jpg\n",
      "(1817, 150, 150)\n",
      "(1817, 1)\n"
     ]
    }
   ],
   "source": [
    "maxData = 1823\n",
    "X_data, Y_data = dwdii_transforms.load_data(dataPath, imagePath, maxData = maxData, verboseFreq = 200, imgResize=imgResize)\n",
    "print X_data.shape\n",
    "print Y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "In this section, we will apply transformations to the existing images to increase of training data, as well as add a bit of noise in the hopes of improving the overall training activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16353, 150, 150)\n",
      "(16353, 1)\n"
     ]
    }
   ],
   "source": [
    "newImgs = np.zeros([X_data.shape[0] * 9, X_data.shape[1], X_data.shape[2]])\n",
    "newYs = np.zeros([Y_data.shape[0] * 9, Y_data.shape[1]], dtype=np.int8)\n",
    "print newImgs.shape\n",
    "print newYs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Done', '2016-12-11 19:57:27.021505')\n"
     ]
    }
   ],
   "source": [
    "ndx = 0\n",
    "for i in range(X_data.shape[0]):\n",
    "    img = X_data[i]\n",
    "    \n",
    "    img0 = dwdii_transforms.reflectY(img)\n",
    "    newImgs[ndx] = img0\n",
    "    newYs[ndx] = Y_data[i]\n",
    "    #misc.imsave(\"test0.png\", img0)\n",
    "    ndx += 1\n",
    "    \n",
    "    img1 = dwdii_transforms.cvDilate(img)\n",
    "    newImgs[ndx] = img1\n",
    "    newYs[ndx] = Y_data[i]\n",
    "    #misc.imsave(\"test1.png\", img1)\n",
    "    ndx += 1\n",
    "    \n",
    "    img2 = dwdii_transforms.cvErode(img)\n",
    "    newImgs[ndx] = img2\n",
    "    newYs[ndx] = Y_data[i]\n",
    "    #misc.imsave(\"test2.png\", img2)\n",
    "    ndx += 1\n",
    "    \n",
    "    img3 = dwdii_transforms.cvDilate2(img)\n",
    "    newImgs[ndx] = img3\n",
    "    newYs[ndx] = Y_data[i]\n",
    "    #misc.imsave(\"test3.png\", img3)\n",
    "    ndx += 1    \n",
    "    \n",
    "    #img4 = dwdii_transforms.cvMedianBlur(img)\n",
    "    #newImgs[ndx] = img4\n",
    "    #newYs[ndx] = Y_data[i]\n",
    "    #misc.imsave(\"test4.png\", img4)\n",
    "    #ndx += 1      \n",
    "    \n",
    "    img5 = dwdii_transforms.cvExcessiveSharpening(img)\n",
    "    newImgs[ndx] = img5\n",
    "    newYs[ndx] = Y_data[i]\n",
    "    #misc.imsave(\"test5.png\", img5)\n",
    "    ndx += 1    \n",
    "    \n",
    "    img6 = dwdii_transforms.cvEdgeEnhancement(img)\n",
    "    newImgs[ndx] = img6\n",
    "    newYs[ndx] = Y_data[i]\n",
    "    #misc.imsave(\"test6.png\", img6)\n",
    "    ndx += 1    \n",
    "\n",
    "    img7 = dwdii_transforms.cvBlurMotion1(img)\n",
    "    newImgs[ndx] = img7\n",
    "    newYs[ndx] = Y_data[i]\n",
    "    #misc.imsave(\"test7.png\", img7)\n",
    "    ndx += 1    \n",
    "    \n",
    "    img8 = dwdii_transforms.cvBlurMotion2(img)\n",
    "    newImgs[ndx] = img8\n",
    "    newYs[ndx] = Y_data[i]\n",
    "    #misc.imsave(\"test8.png\", img8)\n",
    "    ndx += 1      \n",
    "\n",
    "    img9 = dwdii_transforms.reflectY(img)\n",
    "    newImgs[ndx] = img9\n",
    "    newYs[ndx] = Y_data[i]\n",
    "    #misc.imsave(\"test9.png\", img9)\n",
    "    ndx += 1      \n",
    "\n",
    "    #break\n",
    "    \n",
    "print(\"Done\", str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18170, 150, 150)\n",
      "(18170, 1)\n"
     ]
    }
   ],
   "source": [
    "X_data2 = np.concatenate((X_data, newImgs))\n",
    "Y_data2 = np.concatenate((Y_data, newYs))\n",
    "print X_data2.shape\n",
    "print Y_data2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training/Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code segment splits the data into training and test data sets. Currently this is a standard 80/20 split for training and test respectively after performing a random shuffle using the `unison_shuffled_copies` help method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skippedTransforms = False\n",
    "if skippedTransforms:\n",
    "    X_data2 = X_data\n",
    "    Y_data2 = Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14536\n",
      "(14536, 150, 150)\n",
      "(3634, 150, 150)\n",
      "(14536, 1)\n",
      "(3634, 1)\n"
     ]
    }
   ],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    \"\"\"http://stackoverflow.com/a/4602224/2604144\"\"\"\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "# First shuffle the data \n",
    "X_data2, Y_data2 = unison_shuffled_copies(X_data2, Y_data2)\n",
    "\n",
    "# Split the data into Training and Test sets\n",
    "trainNdx = int(X_data2.shape[0] * .8)\n",
    "print trainNdx\n",
    "X_train, X_test = np.split(X_data2, [trainNdx])\n",
    "Y_train, Y_test = np.split(Y_data2, [trainNdx])\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "\n",
    "print Y_train.shape\n",
    "print Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model\n",
    "In this section, we define the model. The `emotion_model` module contains the model definition itself. `emotion_model_v1` is a basic convolutional neural network while our final model is a variation on a model shared by James Hamski (jh).\n",
    "\n",
    "The model is trained on 1 of 8 emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 6, 'neutral': 2, 'contempt': 7, 'disgust': 1, 'anger': 0, 'surprise': 4, 'fear': 5, 'happiness': 3}\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Map the emotions to integers for categorization later.\n",
    "emotions = dwdii_transforms.emotionNumerics()\n",
    "print emotions\n",
    "print len(emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is a convolutional neural network with 4 hidden layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)    (None, 32, 143, 143)2080        convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)          (None, 32, 143, 143)0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)      (None, 32, 71, 71)  0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)    (None, 32, 67, 67)  25632       maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)          (None, 32, 67, 67)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)      (None, 32, 33, 33)  0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)    (None, 64, 31, 31)  18496       maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)          (None, 64, 31, 31)  0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)      (None, 64, 15, 15)  0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)    (None, 64, 14, 14)  16448       maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)          (None, 64, 14, 14)  0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)      (None, 64, 7, 7)    0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)                (None, 3136)        0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                    (None, 64)          200768      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)          (None, 64)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                    (None, 8)           520         activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)          (None, 8)           0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 263944\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#model = emotion_model.emotion_model_v3_2(len(emotions), verbose=True)\n",
    "model = emotion_model.emotion_model_jh_v5(len(emotions), verbose=True, \n",
    "                                        input_shape=(1,X_train.shape[1],X_train.shape[2]))\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code segment trains the model using the `run_network` helper function. Previously, I was hitting a memory issue (my interpretation), when I have batches above a certain threshold. Batches=10 work fine, but batches of 100 are too big. May need to allocate more RAM to the docker container. I have since moved to a Docker Cloud / Amazon Web Services instance with increased memory and this issue has been mitigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reshape to the appropriate shape for the CNN input\n",
    "testX = X_test.reshape(X_test.shape[0], 1, X_train.shape[1],X_train.shape[2])\n",
    "trainX = X_train.reshape(X_train.shape[0], 1, X_train.shape[1],X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loadWeights = False\n",
    "if loadWeights:\n",
    "    model.load_weights(\"dwdii-emo-01vjh-1-Cloud.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start: 2016-12-11 20:04:39.495916\n",
      "(14536, 8)\n",
      "(3634, 8)\n",
      "Training model...\n",
      "Train on 14536 samples, validate on 3634 samples\n",
      "Epoch 1/100\n",
      "14536/14536 [==============================] - 952s - loss: 0.3321 - acc: 0.3789 - val_loss: 0.2828 - val_acc: 0.5146\n",
      "Epoch 2/100\n",
      "14536/14536 [==============================] - 956s - loss: 0.2485 - acc: 0.5474 - val_loss: 0.2280 - val_acc: 0.5806\n",
      "Epoch 3/100\n",
      "14536/14536 [==============================] - 957s - loss: 0.2108 - acc: 0.6278 - val_loss: 0.2225 - val_acc: 0.5897\n",
      "Epoch 4/100\n",
      "14536/14536 [==============================] - 961s - loss: 0.1835 - acc: 0.6835 - val_loss: 0.1784 - val_acc: 0.6973\n",
      "Epoch 5/100\n",
      "14536/14536 [==============================] - 959s - loss: 0.1535 - acc: 0.7405 - val_loss: 0.1616 - val_acc: 0.7320\n",
      "Epoch 6/100\n",
      "14536/14536 [==============================] - 961s - loss: 0.1282 - acc: 0.7915 - val_loss: 0.1268 - val_acc: 0.7917\n",
      "Epoch 7/100\n",
      "14536/14536 [==============================] - 961s - loss: 0.1053 - acc: 0.8304 - val_loss: 0.1251 - val_acc: 0.7939\n",
      "Epoch 8/100\n",
      "14536/14536 [==============================] - 962s - loss: 0.0844 - acc: 0.8678 - val_loss: 0.0881 - val_acc: 0.8660\n",
      "Epoch 9/100\n",
      "14536/14536 [==============================] - 960s - loss: 0.0631 - acc: 0.9017 - val_loss: 0.1036 - val_acc: 0.8401\n",
      "Epoch 10/100\n",
      "14536/14536 [==============================] - 961s - loss: 0.0491 - acc: 0.9278 - val_loss: 0.0722 - val_acc: 0.8963\n",
      "Epoch 11/100\n",
      "14536/14536 [==============================] - 960s - loss: 0.0425 - acc: 0.9359 - val_loss: 0.0643 - val_acc: 0.9122\n",
      "Epoch 12/100\n",
      "14536/14536 [==============================] - 962s - loss: 0.0301 - acc: 0.9557 - val_loss: 0.0662 - val_acc: 0.9122\n",
      "Epoch 13/100\n",
      "14536/14536 [==============================] - 961s - loss: 0.0255 - acc: 0.9641 - val_loss: 0.0667 - val_acc: 0.9235\n",
      "Epoch 14/100\n",
      "14536/14536 [==============================] - 960s - loss: 0.0250 - acc: 0.9688 - val_loss: 0.1090 - val_acc: 0.8591\n",
      "Epoch 15/100\n",
      "14536/14536 [==============================] - 962s - loss: 0.0228 - acc: 0.9685 - val_loss: 0.0537 - val_acc: 0.9307\n",
      "Epoch 16/100\n",
      "14536/14536 [==============================] - 966s - loss: 0.0173 - acc: 0.9770 - val_loss: 0.0601 - val_acc: 0.9207\n",
      "Epoch 17/100\n",
      "14536/14536 [==============================] - 966s - loss: 0.0185 - acc: 0.9780 - val_loss: 0.1397 - val_acc: 0.8558\n",
      "Epoch 18/100\n",
      "14536/14536 [==============================] - 964s - loss: 0.0172 - acc: 0.9803 - val_loss: 0.0541 - val_acc: 0.9400\n",
      "Epoch 19/100\n",
      "14536/14536 [==============================] - 966s - loss: 0.0122 - acc: 0.9844 - val_loss: 0.0708 - val_acc: 0.9229\n",
      "Epoch 20/100\n",
      "14536/14536 [==============================] - 964s - loss: 0.0135 - acc: 0.9851 - val_loss: 0.0628 - val_acc: 0.9406\n",
      "Epoch 21/100\n",
      "14536/14536 [==============================] - 966s - loss: 0.0157 - acc: 0.9874 - val_loss: 0.0531 - val_acc: 0.9461\n",
      "Epoch 22/100\n",
      "14536/14536 [==============================] - 962s - loss: 0.0171 - acc: 0.9864 - val_loss: 0.0620 - val_acc: 0.9450\n",
      "Epoch 23/100\n",
      "14536/14536 [==============================] - 961s - loss: 0.0163 - acc: 0.9866 - val_loss: 0.0636 - val_acc: 0.9367\n",
      "Epoch 24/100\n",
      "14536/14536 [==============================] - 960s - loss: 0.0151 - acc: 0.9856 - val_loss: 0.0644 - val_acc: 0.9425\n",
      "Epoch 25/100\n",
      "14536/14536 [==============================] - 963s - loss: 0.0126 - acc: 0.9856 - val_loss: 0.0624 - val_acc: 0.9419\n",
      "Epoch 26/100\n",
      "14536/14536 [==============================] - 960s - loss: 0.0101 - acc: 0.9884 - val_loss: 0.0665 - val_acc: 0.9411\n",
      "Epoch 27/100\n",
      "14536/14536 [==============================] - 961s - loss: 0.0101 - acc: 0.9908 - val_loss: 0.0941 - val_acc: 0.9265\n",
      "Epoch 28/100\n",
      "  200/14536 [..............................] - ETA: 890s - loss: 0.0087 - acc: 0.9900 KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "print \"Training start: \" + str(datetime.datetime.now())\n",
    "m, h = emotion_model.run_network([trainX, testX, Y_train, Y_test], model, batch=200, epochs=100, verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model v2: Network's test score [loss, accuracy]: [9.1067240715026863, 0.435] - 4474s \n",
    "  * #2: Network's test score [loss, accuracy]: [9.1067240715026863, 0.435] - 5469s \n",
    "* Model v3: Network's test score [loss, accuracy]: [8.0187525367736825, 0.50249999999999995] - 2499s\n",
    "* Model v4.1: Network's test score [loss, accuracy]: [9.1067240715026863, 0.435] - 2137s \n",
    "* Movel v4.2: Network's test score [loss, accuracy]: [8.0187525367736825, 0.50249999999999995] - 3108s \n",
    "* Model v5: Network's test score [loss, accuracy]: [6.9666682052612305, 0.19500000000000001] - 1682s \n",
    "* Model v6: Network's test score [loss, accuracy]: [1.7120025205612182, 0.23749999999999999] - 3020s \n",
    "* Model v7: Network's test score [loss, accuracy]: [7.9999716758728026, 0.14000000000000001] - 4610s \n",
    "* Model v8: Network's test score [loss, accuracy]: [1.6948303937911988, 0.19500000000000001] - 3313s \n",
    "* Model v6 w/ flatten: Network's test score [loss, accuracy]: [7.1107604598999021, 0.17249999999999999] - 3044s \n",
    "* Model v6 (Docker Cloud): Network's test score [loss, accuracy]: [11.153776299942534, 0.307974335472044] - 3597s \n",
    "* Model v3.1: Network's test score [loss, accuracy]: [1.5994336946608279, 0.22314049603196873] - 687s \n",
    "* Model v3.2: Network's test score [loss, accuracy]: [1.5697537031802502, 0.35989011021760792] - 772s\n",
    "* cnn_model_jhamski: Network's test score [loss, accuracy]: [0.31684105933367551, 0.34615384648134423] - 636s\n",
    " * Many epochs 150x150: Network's test score [loss, accuracy]: [0.51708218340690315, 0.6428571428571429]\n",
    "* emotion_model_jh_v2: Network's test score [loss, accuracy]: [0.27507745529690836, 0.55616438421484538] - 2027s\n",
    "  * Epoch x20: Network's test score [loss, accuracy]: [0.32478914950808435, 0.63287671265536793]\n",
    "* v3 epoch x40: Network's test score [loss, accuracy]: [0.078623215722688183, 0.88359303391384048]\n",
    "  * +20: Network's test score [loss, accuracy]: [0.070701496646681364, 0.91750687442713108]\n",
    "* v4 Epoch 20: Network's test score [loss, accuracy]: [0.23540275704827893, 0.61279229702942961]\n",
    "  * +80: Network's test score [loss, accuracy]: [0.1823677838099789, 0.8466299859988804]\n",
    "* v5 Epoch 20: Network's test score [loss, accuracy]: [0.13165531713295181, 0.78610729039781191]\n",
    "  * +80: Network's test score [loss, accuracy]: [0.097839370133615211, 0.9312242091603915]\n",
    "* v5 Epoch 27 - 13082 examples (8 transforms): val_loss: 0.1024 - val_acc: 0.9242\n",
    "* v5 Epoch 27 - 14536 examples (9 transforms): val_loss: 0.0941 - val_acc: 0.9265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"dwdii-emo-150-jhv5-9tf-Cloud.hdf5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Precision & Recall\n",
    "\n",
    "In this section we compute Precision and Recall metrics for each of the emotion classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictOutput = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.46960699e-09,   4.32014586e-14,   9.98015821e-01,\n",
       "         5.17242484e-07,   5.25253341e-10,   1.04667470e-06,\n",
       "         1.73934808e-07,   1.98241440e-03])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictOutput[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neutral': defaultdict(<type 'int'>, {}), 'sadness': defaultdict(<type 'int'>, {}), 'happiness': defaultdict(<type 'int'>, {}), 'disgust': defaultdict(<type 'int'>, {}), 'anger': defaultdict(<type 'int'>, {}), 'surprise': defaultdict(<type 'int'>, {}), 'fear': defaultdict(<type 'int'>, {}), 'contempt': defaultdict(<type 'int'>, {})}\n",
      "{0: 'anger', 1: 'disgust', 2: 'neutral', 3: 'happiness', 4: 'surprise', 5: 'fear', 6: 'sadness', 7: 'contempt'}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "prMetrics = {}\n",
    "\n",
    "# For each emotion\n",
    "for e in emotions.keys():\n",
    "    prMetrics[e] = collections.defaultdict(int)   \n",
    "print prMetrics\n",
    "\n",
    "numEmo = dwdii_transforms.numericEmotions()\n",
    "print numEmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each predicted image\n",
    "for i in range(len(predictOutput)):\n",
    "\n",
    "    arPred = np.array(predictOutput[i])\n",
    "    predictionProb = arPred.max()\n",
    "    predictionNdx = arPred.argmax()\n",
    "    predictedEmo = numEmo[predictionNdx]\n",
    "\n",
    "    # True Positives\n",
    "    if predictionNdx == Y_test[i]:\n",
    "        prMetrics[predictedEmo][\"TruePos\"] += 1.0\n",
    "    # False Positives\n",
    "    else:\n",
    "        prMetrics[predictedEmo][\"FalsePos\"] += 1.0\n",
    "        \n",
    "# Look for false negatives\n",
    "for i in range(len(Y_test)):\n",
    "    \n",
    "    arPred = np.array(predictOutput[i])\n",
    "    predictionProb = arPred.max()\n",
    "    predictionNdx = arPred.argmax()\n",
    "    predictedEmo = numEmo[predictionNdx]\n",
    "    \n",
    "    \n",
    "    yEmo = numEmo[int(Y_test[i])]\n",
    "    if Y_test[i] == predictionNdx:\n",
    "        # Ok\n",
    "        pass\n",
    "    else:\n",
    "        prMetrics[yEmo][\"FalseNeg\"] += 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': defaultdict(int,\n",
       "             {'FalseNeg': 80.0, 'FalsePos': 56.0, 'TruePos': 384.0}),\n",
       " 'contempt': defaultdict(int,\n",
       "             {'FalseNeg': 6.0, 'FalsePos': 8.0, 'TruePos': 13.0}),\n",
       " 'disgust': defaultdict(int,\n",
       "             {'FalseNeg': 10.0, 'FalsePos': 1.0, 'TruePos': 19.0}),\n",
       " 'fear': defaultdict(int, {'FalseNeg': 6.0, 'FalsePos': 1.0, 'TruePos': 18.0}),\n",
       " 'happiness': defaultdict(int,\n",
       "             {'FalseNeg': 223.0, 'FalsePos': 10.0, 'TruePos': 802.0}),\n",
       " 'neutral': defaultdict(int,\n",
       "             {'FalseNeg': 177.0, 'FalsePos': 105.0, 'TruePos': 936.0}),\n",
       " 'sadness': defaultdict(int,\n",
       "             {'FalseNeg': 39.0, 'FalsePos': 54.0, 'TruePos': 222.0}),\n",
       " 'surprise': defaultdict(int,\n",
       "             {'FalseNeg': 13.0, 'FalsePos': 319.0, 'TruePos': 686.0})}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision by Emotion\n",
      "--------------------\n",
      "sadness 0.804347826087\n",
      "neutral 0.899135446686\n",
      "contempt 0.619047619048\n",
      "disgust 0.95\n",
      "anger 0.872727272727\n",
      "surprise 0.682587064677\n",
      "fear 0.947368421053\n",
      "happiness 0.987684729064\n",
      "\n",
      "Recall by Emotion\n",
      "--------------------\n",
      "sadness 0.850574712644\n",
      "neutral 0.840970350404\n",
      "contempt 0.684210526316\n",
      "disgust 0.655172413793\n",
      "anger 0.827586206897\n",
      "surprise 0.981402002861\n",
      "fear 0.75\n",
      "happiness 0.78243902439\n"
     ]
    }
   ],
   "source": [
    "emotionPrecision = {}\n",
    "emotionRecall = {}\n",
    "for p in prMetrics:\n",
    "    emotionPrecision[p] = prMetrics[p][\"TruePos\"] / ( prMetrics[p][\"TruePos\"] + prMetrics[p][\"FalsePos\"])\n",
    "    emotionRecall[p] = prMetrics[p][\"TruePos\"] /( prMetrics[p][\"TruePos\"] + prMetrics[p][\"FalseNeg\"])\n",
    "    \n",
    "print \"Precision by Emotion\"\n",
    "print \"--------------------\"\n",
    "for e in emotionPrecision:\n",
    "    print e, emotionPrecision[e]\n",
    "print\n",
    "print \"Recall by Emotion\"\n",
    "print \"--------------------\"\n",
    "for e in emotionRecall:\n",
    "    print e, emotionRecall[e]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References\n",
    "\n",
    "* OpenCV/CV2: http://askubuntu.com/questions/447409/how-to-install-opencv-2-9-for-python\n",
    "* Docker Commit: http://stackoverflow.com/questions/19585028/i-lose-my-data-when-the-container-exits\n",
    "  * docker ps -l\n",
    "  * docker commit <ContainerID> dittenhafer/dl\n",
    "* http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
