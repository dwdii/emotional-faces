{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA622 Final Project\n",
    "Daniel Dittenhafer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook proceeds with using the trained convolutional neural network to predict emotion for each of the face images in the provided test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import emotion_model\n",
    "import dwdii_transforms\n",
    "\n",
    "random.seed(20275)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was trained on 150x150 images, therefore we will use this size of the test data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgDimension = []\n",
    "imgDimension.append(150)\n",
    "imgDimension.append(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Kaggle Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDataPath = \"../../facial_expressions/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000: y38.jpg\n",
      "0.069930: 183a.jpg\n",
      "0.139860: RamyaKrishna_71.jpg\n",
      "0.209790: NA.NE2.200.jpg\n",
      "0.279720: Brahmanandam_46.jpg\n",
      "0.349650: 189a.jpg\n",
      "0.419580: 89a.jpg\n",
      "0.489510: 43a.jpg\n",
      "0.559441: YM.SU2.59.jpg\n",
      "0.629371: 37a.jpg\n",
      "0.699301: y29.jpg\n",
      "0.769231: 54b.jpg\n",
      "0.839161: Brahmanandam_122.jpg\n",
      "0.909091: Brahmanandam_24.jpg\n",
      "0.979021: NM.SA2.99.jpg\n"
     ]
    }
   ],
   "source": [
    "testData, fileList = dwdii_transforms.load_test_data(testDataPath, imgResize=imgDimension, verboseFreq=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286, 150, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testX = testData.reshape(testData.shape[0], 1, testData.shape[1],testData.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 6, 'neutral': 2, 'contempt': 7, 'disgust': 1, 'anger': 0, 'surprise': 4, 'fear': 5, 'happiness': 3}\n",
      "8\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_5 (Convolution2D)    (None, 32, 143, 143)2080        convolution2d_input_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)          (None, 32, 143, 143)0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)      (None, 32, 71, 71)  0           activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)    (None, 32, 67, 67)  25632       maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)          (None, 32, 67, 67)  0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_6 (MaxPooling2D)      (None, 32, 33, 33)  0           activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)    (None, 64, 31, 31)  18496       maxpooling2d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)          (None, 64, 31, 31)  0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_7 (MaxPooling2D)      (None, 64, 15, 15)  0           activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)    (None, 64, 14, 14)  16448       maxpooling2d_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)         (None, 64, 14, 14)  0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_8 (MaxPooling2D)      (None, 64, 7, 7)    0           activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)                (None, 3136)        0           maxpooling2d_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                    (None, 64)          200768      flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)         (None, 64)          0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                    (None, 8)           520         activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)         (None, 8)           0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 263944\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Map the emotions to integers for categorization later.\n",
    "emotions = dwdii_transforms.emotionNumerics()\n",
    "print emotions\n",
    "print len(emotions)\n",
    "\n",
    "model = emotion_model.emotion_model_jh_v5(len(emotions), verbose=True, \n",
    "                                        input_shape=(1,imgDimension[0],imgDimension[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadWeights = True\n",
    "if loadWeights:\n",
    "    #model.load_weights(\"dwdii-emo-150-jhv5a-Cloud.hdf5\")\n",
    "    model.load_weights(\"dwdii-emo-150-jhv5-9tf-Cloud.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286/286 [==============================] - 6s     \n"
     ]
    }
   ],
   "source": [
    "predictOutput = model.predict(testX, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many predictions?\n",
    "len(predictOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.88955162e-11   8.26386519e-18   9.99909699e-01   6.64880873e-10\n",
      "   6.71791713e-05   7.65165112e-14   2.31399081e-05   1.72253852e-20]\n"
     ]
    }
   ],
   "source": [
    "# Show an example of the prediction output\n",
    "print predictOutput[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predictOutput[0]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numEmo = dwdii_transforms.numericEmotions()\n",
    "numEmo[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y38.jpg'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileList[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist Predictions\n",
    "\n",
    "In this section we save the prediction results to a file in the prescribed Kaggle competition format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each prediction output, build the row for the CSV and add to our list for saving.\n",
    "outputData = []\n",
    "outputData.append([\"Image\", \"Emotion\"])\n",
    "for i in range(len(predictOutput)):\n",
    "    arPred = np.array(predictOutput[i])\n",
    "    predictionProb = arPred.max()\n",
    "    predictionNdx = arPred.argmax()\n",
    "    predictedEmo = numEmo[predictionNdx]\n",
    "    fileName = fileList[i]\n",
    "    outputData.append([fileName,predictedEmo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Image', 'Emotion'], ['y38.jpg', 'neutral'], ['KA.AN2.40.jpg', 'surprise'], ['KajalAgarwal_22.jpg', 'surprise'], ['Jayamadhuri_172.jpg', 'neutral'], ['FaridaJalal_149.jpg', 'happiness'], ['NA.HA2.203.jpg', 'surprise'], ['KM.FE2.24.jpg', 'surprise'], ['JayaBhaduri_42.jpg', 'neutral'], ['FaridaJalal_315.jpg', 'happiness']]\n"
     ]
    }
   ],
   "source": [
    "print outputData[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../output/dwdii_predictions.csv', 'w') as mycsvfile:\n",
    "    dw = csv.writer(mycsvfile)\n",
    "    for row in outputData:\n",
    "        dw.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Sanity Check Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "path = testDataPath + \"/JayaBhaduri_42.jpg\"\n",
    "img = misc.imread(path)\n",
    "img = misc.imresize(img, (150, 150))\n",
    "\n",
    "misc.imsave(\"test1000.jpg\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
